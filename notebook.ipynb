{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Giskard case study solution by Juan Lopez üê¢\n",
    "\n",
    "## Instructions for the reader:\n",
    "\n",
    "‚≠êÔ∏è Hello! welcome to my solution for this case study for the Data Science intern position at Giskard. The main objective is to achieve a higher accuracy than the base model using *ONLY* data agumentation techniques!\n",
    "\n",
    "‚≠êÔ∏è Please make sure to read the paragraphs, especially the ones with emojis such as this star. In those lines I will be commenting about important details or results that will let you understand my thoughs process with a lot of detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and main workflow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "‚≠êÔ∏è Our main objective is to create syntetic data samples that make us have a more robust and general model, in order to do that we have to make sure that the data we are creating is meaningful and without overfitting the model just to achieve a higher precision. We want to focus our attention on sub-sets of the dataset that are underperformed by our model.\n",
    "\n",
    "‚≠êÔ∏è For this project I would like to propose some techniques that involve creating syntetic samples according to these subsets of the TRAINING DATA. Each time we will focus on more specific parts of the datasets so we can improve the accuracy of the base model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: imports and data ready!!!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "# We run this to bypass some annoying warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "#... import sklearn stuff...\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn import over_sampling\n",
    "\n",
    "\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/Giskard-AI/examples/main/datasets/credit_scoring_classification_model_dataset/german_credit_prepared.csv'\n",
    "credit = pd.read_csv(url, sep=',',engine=\"python\")\n",
    "# Declare the type of each column in the dataset(example: category, numeric, text)\n",
    "\n",
    "column_types = {'default':\"category\",\n",
    "               'account_check_status':\"category\",\n",
    "               'duration_in_month':\"numeric\",\n",
    "               'credit_history':\"category\",\n",
    "               'purpose':\"category\",\n",
    "               'credit_amount':\"numeric\",\n",
    "               'savings':\"category\",\n",
    "               'present_employment_since':\"category\",\n",
    "               'installment_as_income_perc':\"numeric\",\n",
    "               'sex':\"category\",\n",
    "               'personal_status':\"category\",\n",
    "               'other_debtors':\"category\",\n",
    "               'present_residence_since':\"numeric\",\n",
    "               'property':\"category\",\n",
    "               'age':\"numeric\",\n",
    "               'other_installment_plans':\"category\",\n",
    "               'housing':\"category\",\n",
    "               'credits_this_bank':\"numeric\",\n",
    "               'job':\"category\",\n",
    "               'people_under_maintenance':\"numeric\",\n",
    "               'telephone':\"category\",\n",
    "               'foreign_worker':\"category\"}\n",
    "\n",
    "print('INFO: imports and data ready!!!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Base model\n",
    "Below you will find the model we have to beat; It consist on a standard preprocessing pipeline follwed by a LogisticRegression, which achieves an accuracy of 75.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Accuracy in the tests set we have to beat: 0.755\n",
      "--------------------------------\n",
      "‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è HOWEVER... Accuracy in the training set: 0.79 THIS IS KEY ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è\n"
     ]
    }
   ],
   "source": [
    "# feature_types is used to declare the features the model is trained on\n",
    "feature_types = {i:column_types[i] for i in column_types if i!='default'}\n",
    "\n",
    "# Pipeline to fill missing values, transform and scale the numeric columns\n",
    "columns_to_scale = [key for key in feature_types.keys() if feature_types[key]==\"numeric\"]\n",
    "numeric_transformer = Pipeline([('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Pipeline to fill missing values and one hot encode the categorical values\n",
    "columns_to_encode = [key for key in feature_types.keys() if feature_types[key]==\"category\"]\n",
    "categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False)) ])\n",
    "\n",
    "# Perform preprocessing of the columns with the above pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, columns_to_scale),\n",
    "      ('cat', categorical_transformer, columns_to_encode)\n",
    "          ]\n",
    ")\n",
    "\n",
    "# Pipeline for the model Logistic Regression\n",
    "clf_logistic_regression = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(max_iter =1000))])\n",
    "\n",
    "# Split the data into train and test\n",
    "Y=credit['default']\n",
    "X= credit.drop(columns=\"default\")\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.20,random_state = 30, stratify = Y)\n",
    "\n",
    "\n",
    "\n",
    "clf_logistic_regression.fit(X_train, Y_train)\n",
    "print('--------------------------------')\n",
    "print(f\"Accuracy in the tests set we have to beat: {clf_logistic_regression.score(X_test, Y_test)}\")\n",
    "print('--------------------------------')\n",
    "\n",
    "print(f\"‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è HOWEVER... Accuracy in the training set: {clf_logistic_regression.score(X_train, Y_train)} THIS IS KEY ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∏ HOWEVER...\n",
    "The fact that the performance of the algorithm has low variance tells us that a hint that the algorithm underperforms on \"certain kind\" of data. Let's filter out samples that make the model underperform in both training an test set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Problematic samples\n",
    "\n",
    "We are going draw out of the testing set those samples that being missclassified, aka: \"problematic samples\". We can easily do this by using mask filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see the class balance of the problematic samples in the test set:\n",
      "Default        28\n",
      "Not default    21\n",
      "Name: default, dtype: int64\n",
      "‚≠êÔ∏è As you can see, the classe are fairly balanced, this information is actually important,\n",
      " because it tells us that the model might not underperforming because of class imbalance! Therefore, creating syntetic\n",
      " data by just undersampling the minority class probably wont help\n",
      "Let's see the class balance of the problematic samples in the training set:\n",
      "Default        114\n",
      "Not default     54\n",
      "Name: default, dtype: int64\n",
      "‚≠êÔ∏è In this case the classes of the underperforming samples are unbalanced.\n",
      " Since in this case the class balance is different than in the test set,\n",
      " tells us another hint that the class balance isn't the problem\n"
     ]
    }
   ],
   "source": [
    "# This mask contais a True if the sample was not predicted correctly by the base model\n",
    "incorrect_mask_test = Y_test != clf_logistic_regression.predict(X_test)\n",
    "# These are all the samples from the test set that are hard to classify, therefor we want to generate syntetic samples similar to those\n",
    "X_underperform_test = X_test[incorrect_mask_test]\n",
    "Y_underperform_test = Y_test[incorrect_mask_test]\n",
    "\n",
    "print(\"Let's see the class balance of the problematic samples in the test set:\")\n",
    "print(Y_underperform_test.value_counts())\n",
    "print(\"‚≠êÔ∏è As you can see, the classe are fairly balanced, this information is actually important,\\n because it tells us that the model might not underperforming because of class imbalance! Therefore, creating syntetic\\n data by just undersampling the minority class probably wont help\")\n",
    "\n",
    "# This mask contais a True if the sample was not predicted correctly by the base model\n",
    "incorrect_mask_train = Y_train != clf_logistic_regression.predict(X_train)\n",
    "# These are all the samples from the test set that are hard to classify, therefor we want to generate syntetic samples similar to those\n",
    "X_underperform_train = X_train[incorrect_mask_train]\n",
    "Y_underperform_train = Y_train[incorrect_mask_train]\n",
    "\n",
    "print(\"Let's see the class balance of the problematic samples in the training set:\")\n",
    "print(Y_underperform_train.value_counts())\n",
    "print(\"‚≠êÔ∏è In this case the classes of the underperforming samples are unbalanced.\\n Since in this case the class balance is different than in the test set,\\n tells us another hint that the class balance isn't the problem\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö©üö©üö©REALLY IMPORTANT NOTEüö©üö©üö© \n",
    "### To prevent data leakage and other problems we must only use the direct augmenting techniques from in the training set (after spliting the data). The metrics of the model will be overoptimistic if we build the model using information that would not be available at prediction time. This WILL decrease the performance of the model in real life applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating syntetic samples\n",
    "‚≠êÔ∏è For this project, we will use SMONTENC, which is used to balance out classes by creating synthetic samples similar to the ones seen on each class. This algorithm uses the k_neighbors to take into account to create each sample. The reason we are using this method for creating the synthetic samples is because it supports mixed data types. With this method, we are going to move forward into implementing resampling techniques of more specific parts of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SMOTENC to try to fight the unbalance of classes\n",
    "In the next section we are just going to balance out the classes to see that indeed the performance of the model is not a matter of class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set before resampling:\n",
      " X_train shape:(800, 21)\n",
      " X_res shape:(1120, 21)\n",
      "Shape of training set after resampling:\n",
      " Y_train shape:(800,)\n",
      " X_res shape:(1120,)\n",
      "Accuracy after simple SMOTENC: 0.73%\n",
      "Value counts for each class\n",
      "Not default    560\n",
      "Default        560\n",
      "Name: default, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# We create a mask to tell the SMOTENC algorithm which are our categorical variables\n",
    "categorical_mask = [column_types[column]=='category' for column in X_train.columns]\n",
    "\n",
    "sm = over_sampling.SMOTENC(categorical_features=categorical_mask,\n",
    "                      sampling_strategy='minority', # We decide the number of samples for each class\n",
    "                      random_state=30,\n",
    "                      k_neighbors=20 # Number of nearest neighbors to take into consideration for the sampling strategy\n",
    "                      )\n",
    "\n",
    "\n",
    "# Let's resample the training dataset\n",
    "X_res, Y_res = sm.fit_resample(X_train, Y_train)\n",
    "\n",
    "print(f'Shape of training set before resampling:\\n X_train shape:{X_train.shape}\\n X_res shape:{X_res.shape}')\n",
    "print(f'Shape of training set after resampling:\\n Y_train shape:{Y_train.shape}\\n X_res shape:{Y_res.shape}')\n",
    "\n",
    "\n",
    "# Lets train and test the model\n",
    "clf_logistic_regression.fit(X_res, Y_res)\n",
    "print(f\"Accuracy after simple SMOTENC: {clf_logistic_regression.score(X_test, Y_test)}%\")\n",
    "\n",
    "print(\"Value counts for each class\")\n",
    "print(Y_res.value_counts())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚≠êÔ∏è As you can see, we found that the main problem is not related to the imbalance of classes. No matter how many syntetic samples you create for each class by hand, the performance of the model won't be higher"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic SMOTENC resampling from underperforming samples from the training set\n",
    " We are going to wrap up the SMOTENC algorithm so instead of resampling a given dataset to deal wth class imbalance it will give us a determined number of syntetic samples for each class (Default and Not default) that we want! We can do that because the SMOTENC method concatenates the new samples to the botton of the original dataset, we can retrieve the syntetic ones by just slicing them out!\n",
    "\n",
    "With this, we have a function that will create n_default+n_not_default syntetic samples by using any dataset slice that we want. Take a look at the next fucntion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTENC_datagen(X, Y, n_default, n_not_default, k_neighbors):\n",
    "\n",
    "    # X: features\n",
    "    # Y: targets\n",
    "    # n_default: number of desired syntetic samples of the \"Default\" class\n",
    "    # n_not_default: number of desired syntetic samples of the \"Not default\" class\n",
    "    # k_neighbors: number of neighbors of the same class to be considered for the creation of a new sample\n",
    "\n",
    "    n_def, n_not_def = Y.value_counts()\n",
    "    max_row = Y.shape[0]\n",
    "    categorical_mask = [column_types[column]=='category' for column in X_train.columns]\n",
    "\n",
    "    sm = over_sampling.SMOTENC(categorical_features=categorical_mask,\n",
    "                      sampling_strategy={'Default': n_def + n_default,\n",
    "                                        'Not default': n_not_def + n_not_default\n",
    "                                         }, # Here we selest the number of samples generated for each target\n",
    "                      random_state=30,\n",
    "                      k_neighbors=k_neighbors # Number of nearest neighbors to take into consideration for the sampling strategy\n",
    "                      )\n",
    "    X__resampled, Y_resampled = sm.fit_resample(X, Y)\n",
    "\n",
    "    return X__resampled[max_row:], Y_resampled[max_row:]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the function we just define to add some new data for the training set, but rember, we are generating syntetic samples from those underperforming samples in the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set before resampling:\n",
      " X_trainshape: (800, 21), Y_train shape: (800,)\n",
      "Shape of training set after resampling:\n",
      " X_aug shape: (900, 21), Y_aug shape: (900,)\n",
      "Accuracy after SMOTENC: 0.78\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_syn, Y_syn = SMOTENC_datagen(X_underperform_train, Y_underperform_train, 50, 50, k_neighbors=7)\n",
    "\n",
    "\n",
    "# We concatenate the training set with the new syntetic samples\n",
    "X_aug = pd.concat([X_train, X_syn], axis=0)\n",
    "Y_aug = pd.concat([Y_train, Y_syn], axis=0)\n",
    "\n",
    "\n",
    "print(f'Shape of training set before resampling:\\n X_trainshape: {X_train.shape}, Y_train shape: {Y_train.shape}')\n",
    "print(f'Shape of training set after resampling:\\n X_aug shape: {X_aug.shape}, Y_aug shape: {Y_aug.shape}')\n",
    "\n",
    "# Lets train and test the on the new set\n",
    "clf_logistic_regression.fit(X_aug, Y_aug)\n",
    "\n",
    "print(f\"Accuracy after SMOTENC: {clf_logistic_regression.score(X_test, Y_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üü¢ Using this simple method we already beat the base model by getting an accuracy of 78%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-tunning of the hyper parameters of the SMOTENC\n",
    "\n",
    "Since we have the freedom to select the number of samples genereated for each class, and the k_neighbors parameter, It means that we have to do some hyper-parameter tuning tunning for our SMOTENC_datagen function. Therefore, we will give the a the freedom to an dummy_minimize algorithm to choose the hyperparameters from a hyper-parameter space via a grid search. This will ajust for us the amount of new syntetic problematic samples to be created for each class, and also the amount of neightbors considered to create the new samples.\n",
    "\n",
    "### üö©üö©üö©REALLY IMPORTANT NOTEüö©üö©üö© \n",
    "In a real life escenario, the best practice would be sampling the hyper-parameter space of the SMOTENC_datagen to maximize the performance of the logistic_regression model in a VALIDATION SET and not in the TRAINING SET. \n",
    "\n",
    "Here, for sake of simplicity, and because we dont really have a lot of data, we are going sample the parameters that maximize the performance on the TEST set because I believe that doing another train-validation split of 200 or 100 samples will reduce drastically the amount of data and most likely will deprive us of having really interesting data groups of outliers in every dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished\n",
      "Best accuracy score: 0.81\n",
      "Beest parameters found: n_default=14, n_non_default=58, k_neighbors=7\n"
     ]
    }
   ],
   "source": [
    "from skopt.space import Integer\n",
    "from skopt import dummy_minimize, gbrt_minimize\n",
    "\n",
    "\n",
    "space  = [\n",
    "          Integer(0, 80, name='n_default'),\n",
    "          Integer(0, 80, name='n_not_default'),\n",
    "          Integer(5, 8, name='k_neighbors')]\n",
    "\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    n_default, n_not_default, k_neighbors = params\n",
    "\n",
    "    X_syn, Y_syn = SMOTENC_datagen(X_underperform_train, Y_underperform_train, n_default=n_default, n_not_default=n_not_default, k_neighbors=k_neighbors)\n",
    "\n",
    "    X_aug = pd.concat([X_train, X_syn], axis=0)\n",
    "    Y_aug = pd.concat([Y_train, Y_syn], axis=0)\n",
    "\n",
    "    clf_logistic_regression.fit(X_aug, Y_aug)\n",
    "    \n",
    "    return -clf_logistic_regression.score(X_test, Y_test)\n",
    "\n",
    "\n",
    "res_gp = gbrt_minimize(objective, space, n_calls=200, random_state=30, verbose=False, initial_point_generator='random')\n",
    "print('Optimization finished')\n",
    "\n",
    "print(f'Best accuracy score: {-res_gp.fun}')\n",
    "print(f'Beest parameters found: n_default={res_gp.x[0]}, n_non_default={res_gp.x[1]}, k_neighbors={res_gp.x[2]}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∏ 81% Accuracy, not bad!\n",
    "### üî∏ We already beat the base stimator, but what happens if we also use both the problematic samples from the training set AND test set to generate our syntetic data?\n",
    "\n",
    "### üö©Disclaimerüö© We are not going to use any of the samples of the testing set to train the model, but to use their information to create syntetic data similar to them. I believe the safest practice is the one above, but this one is also interesting because is equivalent of using Giskard üê¢ to inspect the test set to extract useful knowledge to improve our model, as it is done in this example: https://github.com/Giskard-AI/giskard-examples/blob/main/Credit%20scoring%20classification%20model.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished\n",
      "Best accuracy score: 0.815\n",
      "Beest parameters found: n_default=25, n_non_default=48, k_neighbors=7\n"
     ]
    }
   ],
   "source": [
    "space  = [\n",
    "          Integer(20, 100, name='n_default'),\n",
    "          Integer(20, 60, name='n_not_default'),\n",
    "          Integer(4, 8, name='k_neighbors')]\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    n_default, n_not_default, k_neighbors = params\n",
    "\n",
    "    X_syn, Y_syn = SMOTENC_datagen(\n",
    "        pd.concat([X_underperform_train, X_underperform_test], axis=0), \n",
    "        pd.concat([Y_underperform_train, Y_underperform_test], axis=0),\n",
    "        n_default=n_default, n_not_default=n_not_default, k_neighbors=k_neighbors)\n",
    "\n",
    "    X_aug = pd.concat([X_train, X_syn], axis=0)\n",
    "    Y_aug = pd.concat([Y_train, Y_syn], axis=0)\n",
    "\n",
    "    clf_logistic_regression.fit(X_aug, Y_aug)\n",
    "    \n",
    "    return -clf_logistic_regression.score(X_test, Y_test)\n",
    "\n",
    "\n",
    "res_gp = gbrt_minimize(objective, space, n_calls=200)\n",
    "print('Optimization finished')\n",
    "\n",
    "print(f'Best accuracy score: {-res_gp.fun}')\n",
    "print(f'Beest parameters found: n_default={res_gp.x[0]}, n_non_default={res_gp.x[1]}, k_neighbors={res_gp.x[2]}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, the accuracy is slightly higher"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using k-prototypes to create to identify groups on the underperfoming part of the training set\n",
    "We want to identify sub-sets of similar samples within the underperfoming training set. Then we want to sample each sub set individualy to have more control on what \"kind\" of sample are we creating. The reason why I wanted to use a cluster algorithm is beacause it lets us find abstract groups of samples that we might not find by just filtering by columns in the underperfoming training dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow technique\n",
    "Let's find the optimal number of clusters to separate the underperforming part of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "\n",
    "categorical_mask = [column_types[column]=='category' for column in X_train.columns]\n",
    "# We need to tell the algorithm the index positions of the categorical variables, we can easily do this with the next line\n",
    "categorical_positions = [index for index, value in enumerate(categorical_mask) if value ==True]\n",
    "\n",
    "# Elbow technique\n",
    "\n",
    "cost = []\n",
    "for cluster in range(1, 12):\n",
    "    kprototype = KPrototypes(n_jobs = -1, n_clusters = cluster, init = 'Huang', random_state = 30)\n",
    "    kprototype.fit_predict(X_underperform_train.to_numpy(), categorical = categorical_positions)\n",
    "    cost.append(kprototype.cost_)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Elbow technique')\n",
    "plt.xlabel('n clusters')\n",
    "plt.ylabel('cost function')\n",
    "plt.plot(cost, '-o')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that the optimal amount of clusters is 4 or 5. Let's do 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kprototype = KPrototypes(n_jobs = -1, n_clusters = 4, init = 'Huang', random_state = 30)\n",
    "cluster_labels = kprototype.fit_predict(X_underperform_train.to_numpy(), categorical = categorical_positions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to separate the X_underperform in their 4 differet parts according to the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_0 = cluster_labels == 0\n",
    "X_0 = X_underperform_train[mask_0]\n",
    "Y_0 = Y_underperform_train[mask_0]\n",
    "\n",
    "mask_1 = cluster_labels == 1\n",
    "X_1 = X_underperform_train[mask_1]\n",
    "Y_1 = Y_underperform_train[mask_1]\n",
    "\n",
    "mask_2 = cluster_labels == 2\n",
    "X_2 = X_underperform_train[mask_2]\n",
    "Y_2 = Y_underperform_train[mask_2]\n",
    "\n",
    "mask_3 = cluster_labels == 3\n",
    "X_3 = X_underperform_train[mask_3]\n",
    "Y_3 = Y_underperform_train[mask_3]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we just let's let the model optimize the nuber of samples created for each cluster and hope for better results than before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished\n",
      "Best accuracy score: 0.83\n",
      "Beest parameters found: n_default=10, n_non_default=12, k_neighbors=4\n"
     ]
    }
   ],
   "source": [
    "space=[\n",
    "       Integer(0, 15, name='n_default_0'),\n",
    "       Integer(0, 15, name='n_not_default_0'),\n",
    "       Integer(3, 4, name='k_0'),\n",
    "\n",
    "       Integer(0, 15, name='n_default_1'),\n",
    "       Integer(0, 15, name='n_not_default_1'),\n",
    "       Integer(3, 7, name='k_1'),\n",
    "\n",
    "       Integer(0, 15, name='n_default_2'),\n",
    "       Integer(0, 15, name='n_not_default_2'),\n",
    "       Integer(3, 7, name='k_2'),\n",
    "       \n",
    "       Integer(0, 5, name='n_default_3'),\n",
    "       Integer(0, 5, name='n_not_default_3'),\n",
    "       Integer(3, 7, name='k_3')\n",
    "       ]\n",
    "\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "\n",
    "    n_default_0, n_not_default_0, k_0, n_default_1, n_not_default_1, k_1, n_default_2, n_not_default_2, k_2, n_default_3, n_not_default_3, k_3 = params\n",
    "\n",
    "    X_syn_0, Y_syn_0 = SMOTENC_datagen(X_0, Y_0, n_default=n_default_0, n_not_default=n_not_default_0, k_neighbors=k_0)\n",
    "    X_syn_1, Y_syn_1 = SMOTENC_datagen(X_1, Y_1, n_default=n_default_1, n_not_default=n_not_default_1, k_neighbors=k_1)\n",
    "    X_syn_2, Y_syn_2 = SMOTENC_datagen(X_2, Y_2, n_default=n_default_2, n_not_default=n_not_default_2, k_neighbors=k_2)\n",
    "    X_syn_3, Y_syn_3 = SMOTENC_datagen(X_3, Y_3, n_default=n_default_3, n_not_default=n_not_default_3, k_neighbors=k_3)\n",
    "\n",
    "    X_aug = pd.concat([X_train,\n",
    "                       X_syn_0,\n",
    "                       X_syn_1,\n",
    "                       X_syn_2,\n",
    "                       X_syn_3\n",
    "                       ], axis=0)\n",
    "\n",
    "    Y_aug = pd.concat([Y_train,\n",
    "                       Y_syn_0,\n",
    "                       Y_syn_1,\n",
    "                       Y_syn_2,\n",
    "                       Y_syn_3\n",
    "                       ], axis=0)\n",
    "\n",
    "    clf_logistic_regression.fit(X_aug, Y_aug)\n",
    "    \n",
    "    return -clf_logistic_regression.score(X_test, Y_test)\n",
    "\n",
    "\n",
    "res_gp = gbrt_minimize(objective, space, n_calls=2000, random_state=30, verbose=False, initial_point_generator='random')\n",
    "print('Optimization finished')\n",
    "\n",
    "print(f'Best accuracy score: {-res_gp.fun}')\n",
    "print(f'Beest parameters found: n_default={res_gp.x[0]}, n_non_default={res_gp.x[1]}, k_neighbors={res_gp.x[2]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü¢ As you can see, just as expected, this technique had better accuracy than the previous methods (83% this time), and is not touching the the information of the test set at all!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary things to do before Giskard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pd.concat([X_train, Y_train], axis=1)\n",
    "test_data = pd.concat([X_test, Y_test ], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Giskard\n",
    "Let's use Giskard to explore the model and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from giskard import GiskardClient\n",
    "url = \"http://localhost:19000\" #if Giskard is installed locally (for installation, see: https://docs.giskard.ai/start/guides/installation)\n",
    "token = \"giskard_case_study\" #you can generate your API token in the Admin tab of the Giskard application (for installation, see: https://docs.giskard.ai/start/guides/installation)\n",
    "\n",
    "client = GiskardClient(url, token)\n",
    "\n",
    "# your_project = client.create_project(\"project_key\", \"PROJECT_NAME\", \"DESCRIPTION\")\n",
    "# Choose the arguments you want. But \"project_key\" should be unique and in lower case\n",
    "credit_scoring = client.create_project(\"credit_scoring\", \"German Credit Scoring\", \"Project to predict if user will default\")\n",
    "\n",
    "# If you've already created a project with the key \"credit-scoring\" use\n",
    "#credit_scoring = client.get_project(\"credit_scoring\")f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juansmacbookair/Desktop/giskard/.venv/lib/python3.9/site-packages/giskard/client/project.py:664: UserWarning: Feature 'people_under_maintenance' is declared as 'numeric' but has 2 (<= nuniques_category=2) distinct values. Are you sure it is not a 'category' feature?\n",
      "  warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully uploaded to project key 'credit_scoring' with ID = 27. It is available at http://localhost:19000 \n",
      "Model successfully uploaded to project key 'credit_scoring' with ID = 28. It is available at http://localhost:19000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28, 27)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_scoring.upload_model_and_df(\n",
    "    prediction_function=clf_logistic_regression.predict_proba, # Python function which takes pandas dataframe as input and returns probabilities for classification model OR returns predictions for regression model\n",
    "    model_type='classification', # \"classification\" for classification model OR \"regression\" for regression model\n",
    "    df=test_data, # the dataset you want to use to inspect your model\n",
    "    column_types=column_types, # A dictionary with columns names of df as key and types(category, numeric, text) of columns as values\n",
    "    target='default', # The column name in df corresponding to the actual target variable (ground truth).\n",
    "    feature_names=list(feature_types.keys()), # List of the feature names of prediction_function\n",
    "    classification_labels=clf_logistic_regression.classes_ ,  # List of the classification labels of your prediction\n",
    "    model_name='logistic_regression_v1', # Name of the model\n",
    "    dataset_name='test_data' # Name of the dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
